{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import SimpleRNN\n",
    "from keras import optimizers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "import time\n",
    "import mpi4py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigrams(words):\n",
    "    bigrams = []\n",
    "    for b in words:\n",
    "        bigrams.append([b[i:i+2] for i in range(len(b)-1)])\n",
    "    return bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def prepare(maxlen, dataset_filename='./data/dataset.csv', use_bigram=False):\n",
    "    # df = pd.read_csv('./data/dataset.csv')\n",
    "    df = pd.read_csv(dataset_filename)\n",
    "    X = df['NAME']\n",
    "    y = df['NATIONALITY']\n",
    "    num_classes = len(y.unique())\n",
    "\n",
    "    X_train_df, X_test_df, y_train_df, y_test_df = train_test_split(X, y, test_size=0.2, random_state=69)\n",
    "\n",
    "    X_tokenizer = Tokenizer(num_words=None, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', \n",
    "              lower=False, char_level=True, oov_token=None)\n",
    "\n",
    "    y_tokenizer = Tokenizer(num_words=None, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', \n",
    "              lower=True, char_level=False, oov_token=None)\n",
    "\n",
    "    X_train = X_train_df.values.astype(str) # Otherwise, there's an error when calling 'fit_on_texts' >> AttributeError: 'int' object has no attribute 'lower'\n",
    "    X_test = X_test_df.values.astype(str) # Otherwise, there's an error when calling 'fit_on_texts' >> AttributeError: 'int' object has no attribute 'lower'\n",
    "\n",
    "    if use_bigram:\n",
    "        X_train = bigrams(X_train)\n",
    "\n",
    "    X_tokenizer.fit_on_texts(X_train)\n",
    "    X_train = X_tokenizer.texts_to_sequences(X_train)\n",
    "    X_test = X_tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "    X_train = X_tokenizer.sequences_to_matrix(X_train, mode='tfidf')\n",
    "    X_test = X_tokenizer.sequences_to_matrix(X_test, mode='tfidf')\n",
    "\n",
    "    # encode from string labels to numerical labels \n",
    "    label_encoder = LabelEncoder()\n",
    "    y_train = label_encoder.fit_transform(y_train_df.values.astype(str)) # error without astype(str)\n",
    "    y_test = label_encoder.transform(y_test_df.values.astype(str))\n",
    "\n",
    "    y_train = to_categorical(y_train, num_classes)\n",
    "    y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "    # pad character sequences to have the same length\n",
    "    X_train = sequence.pad_sequences(X_train, padding=\"post\", maxlen=maxlen)\n",
    "    X_test = sequence.pad_sequences(X_test, padding=\"post\", maxlen=maxlen)\n",
    "    \n",
    "    max_features = len(X_tokenizer.word_counts)\n",
    "    \n",
    "    return [X_train, y_train, X_test, y_test, max_features, num_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, y_train,\n",
    "          X_test, y_test,\n",
    "          max_features,\n",
    "          maxlen,\n",
    "          num_classes,\n",
    "          nn_type='simple_rnn',\n",
    "          embedding_dims = 50,\n",
    "          epochs=20,\n",
    "          batch_size = 23,\n",
    "         verbose=0):\n",
    "    \n",
    "#     print('Build model...')\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features,\n",
    "                        embedding_dims,\n",
    "                        input_length=maxlen))\n",
    "    if nn_type == 'simple_rnn':\n",
    "        model.add(SimpleRNN(embedding_dims))\n",
    "    elif nn_type == 'lstm':\n",
    "        model.add(LSTM(maxlen))\n",
    "    \n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "#     print(model.summary())\n",
    "#     print('Train model...')\n",
    "\n",
    "    history = model.fit(X_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(X_test, y_test),\n",
    "                verbose=verbose\n",
    "             )\n",
    "    score, acc = model.evaluate(X_test, y_test,\n",
    "                                batch_size=batch_size,\n",
    "                               verbose=verbose)\n",
    "\n",
    "    print('Test model score:', score)\n",
    "    print('Test model accuracy:', acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 30\n",
    "\n",
    "tuning_list = [\n",
    "    {    'name': 'simple_rnn',\n",
    "        'use_bigram': False,\n",
    "        'maxlen': MAX_LEN,\n",
    "        'nn_type': 'simple_rnn'\n",
    "    },\n",
    "    {\n",
    "        'name': 'lstm',\n",
    "        'use_bigram': False,\n",
    "        'maxlen': MAX_LEN,\n",
    "        'nn_type': 'lstm'\n",
    "    },\n",
    "    {\n",
    "        'name': 'simple_rnn_with_bigram',\n",
    "        'use_bigram': True,\n",
    "        'maxlen': MAX_LEN,\n",
    "        'nn_type': 'simple_rnn'\n",
    "    },\n",
    "    {\n",
    "        'name': 'lstm_with_bigram',\n",
    "        'use_bigram': True,\n",
    "        'maxlen': MAX_LEN,\n",
    "        'nn_type': 'lstm'\n",
    "    }\n",
    "]\n",
    "\n",
    "def main():\n",
    "    for params in tuning_list:\n",
    "        print('##### {} #####'.format(params['name']))\n",
    "        [X_train, y_train, X_test, y_test, max_features, num_classes] = prepare(\n",
    "            maxlen=params['maxlen'], use_bigram=params['use_bigram'])\n",
    "        model(nn_type=params['nn_type'],\n",
    "              X_train=X_train, y_train=y_train, \n",
    "              X_test=X_test, y_test=y_test, \n",
    "              max_features=max_features, \n",
    "              num_classes=num_classes, \n",
    "              maxlen=params['maxlen'], \n",
    "              verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mpi_mpi.py\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "print(\"hello world from process %d/%d\" %(rank, size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
