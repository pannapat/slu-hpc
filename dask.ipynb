{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import SimpleRNN\n",
    "from keras import optimizers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigrams(words):\n",
    "    bigrams = []\n",
    "    for b in words:\n",
    "        bigrams.append([b[i:i+2] for i in range(len(b)-1)])\n",
    "    return bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "5700 train sequences\n1425 test sequences\nPad sequences (samples x time)\nX_train shape: (5700, 30)\nX_test shape: (1425, 30)\nHyperparameters\nmax_features = 54\nbatch_size = 23\nmaxlen = 30\nembedding_dims = 50\nepochs = 20\n"
    }
   ],
   "source": [
    "max_features = len(X_tokenizer.word_counts)\n",
    "# print('max_features = {}'.format(max_features))\n",
    "batch_size = 23\n",
    "maxlen = 30\n",
    "embedding_dims = 50\n",
    "epochs=20\n",
    "\n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "X_train = sequence.pad_sequences(X_train, padding=\"post\", maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, padding=\"post\", maxlen=maxlen)\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "\n",
    "print('Hyperparameters')\n",
    "print('max_features = {}'.format(max_features))\n",
    "print('batch_size = {}'.format(batch_size))\n",
    "print('maxlen = {}'.format(maxlen))\n",
    "print('embedding_dims = {}'.format(embedding_dims))\n",
    "print('epochs = {}'.format(epochs))\n",
    "\n",
    "def train_model():\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features,\n",
    "                        embedding_dims,\n",
    "                        input_length=maxlen))\n",
    "    model.add(SimpleRNN(embedding_dims))\n",
    "\n",
    "#     model.add(LSTM(maxlen))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    print(model.summary())\n",
    "    \n",
    "    print('Train...')\n",
    "\n",
    "    history = model.fit(X_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(X_test, y_test),\n",
    "                verbose=0\n",
    "             )\n",
    "    score, acc = model.evaluate(X_test, y_test,\n",
    "                                batch_size=batch_size,\n",
    "                               verbose=1)\n",
    "\n",
    "\n",
    "    print('Test score:', score)\n",
    "    print('Test accuracy:', acc)\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/dataset.csv')\n",
    "# print('data shape: {}'.format(df.shape))\n",
    "\n",
    "X = df['NAME']\n",
    "y = df['NATIONALITY']\n",
    "\n",
    "classes = y.unique()\n",
    "# print(classes)\n",
    "num_classes = len(y.unique())\n",
    "\n",
    "# print('number of classes: {}'.format(num_classes))\n",
    "\n",
    "X_train_df, X_test_df, y_train_df, y_test_df = train_test_split(X, y, test_size=0.2, random_state=69)\n",
    "\n",
    "# print('train data shape X, y: {},{}'.format(X_train_df.shape, y_train_df.shape))\n",
    "# print('test data shape X, y: {},{}'.format(X_test_df.shape, y_test_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tokenizer = Tokenizer(num_words=None, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', \n",
    "          lower=False, char_level=True, oov_token=None)\n",
    "\n",
    "y_tokenizer = Tokenizer(num_words=None, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', \n",
    "          lower=True, char_level=False, oov_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_df.values.astype(str) # Otherwise, there's an error when calling 'fit_on_texts' >> AttributeError: 'int' object has no attribute 'lower'\n",
    "X_test = X_test_df.values.astype(str) # Otherwise, there's an error when calling 'fit_on_texts' >> AttributeError: 'int' object has no attribute 'lower'\n",
    "\n",
    "# X_train = bigrams(X_train)\n",
    "\n",
    "X_tokenizer.fit_on_texts(X_train)\n",
    "X_train = X_tokenizer.texts_to_sequences(X_train)\n",
    "X_test = X_tokenizer.texts_to_sequences(X_test)\n",
    "# print(len(X_tokenizer.index_word))\n",
    "\n",
    "X_train = X_tokenizer.sequences_to_matrix(X_train, mode='tfidf')\n",
    "X_test = X_tokenizer.sequences_to_matrix(X_test, mode='tfidf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode from string labels to numerical labels \n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train_df.values.astype(str)) # error without astype(str)\n",
    "y_test = label_encoder.transform(y_test_df.values.astype(str))\n",
    "# print(encoder.classes_.shape)\n",
    "# encoder.inverse_transform(y_train)\n",
    "\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "# X_tokenizer.word_counts\n",
    "# print(len(X_tokenizer.word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Build model...\nModel: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (None, 30, 50)            2700      \n_________________________________________________________________\nsimple_rnn_1 (SimpleRNN)     (None, 50)                5050      \n_________________________________________________________________\ndense_1 (Dense)              (None, 3)                 153       \n=================================================================\nTotal params: 7,903\nTrainable params: 7,903\nNon-trainable params: 0\n_________________________________________________________________\nNone\nTrain...\n1425/1425 [==============================] - 0s 65us/step\nTest score: 0.9624769159785488\nTest accuracy: 0.48491227626800537\nCPU times: user 1min 4s, sys: 16.8 s, total: 1min 21s\nWall time: 33.1 s\n"
    }
   ],
   "source": [
    "%time _ = train_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}