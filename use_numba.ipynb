{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import SimpleRNN\n",
    "from keras import optimizers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "import time\n",
    "from numba import jit, njit, prange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigrams(words):\n",
    "    bigrams = []\n",
    "    for b in words:\n",
    "        bigrams.append([b[i:i+2] for i in range(len(b)-1)])\n",
    "    return bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def prepare(maxlen, dataset_filename='./data/dataset.csv', use_bigram=False):\n",
    "    # df = pd.read_csv('./data/dataset.csv')\n",
    "    df = pd.read_csv(dataset_filename)\n",
    "    X = df['NAME']\n",
    "    y = df['NATIONALITY']\n",
    "    num_classes = len(y.unique())\n",
    "\n",
    "    X_train_df, X_test_df, y_train_df, y_test_df = train_test_split(X, y, test_size=0.2, random_state=69)\n",
    "\n",
    "    X_tokenizer = Tokenizer(num_words=None, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', \n",
    "              lower=False, char_level=True, oov_token=None)\n",
    "\n",
    "    y_tokenizer = Tokenizer(num_words=None, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', \n",
    "              lower=True, char_level=False, oov_token=None)\n",
    "\n",
    "    X_train = X_train_df.values.astype(str) # Otherwise, there's an error when calling 'fit_on_texts' >> AttributeError: 'int' object has no attribute 'lower'\n",
    "    X_test = X_test_df.values.astype(str) # Otherwise, there's an error when calling 'fit_on_texts' >> AttributeError: 'int' object has no attribute 'lower'\n",
    "\n",
    "    if use_bigram:\n",
    "        X_train = bigrams(X_train)\n",
    "\n",
    "    X_tokenizer.fit_on_texts(X_train)\n",
    "    X_train = X_tokenizer.texts_to_sequences(X_train)\n",
    "    X_test = X_tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "    X_train = X_tokenizer.sequences_to_matrix(X_train, mode='tfidf')\n",
    "    X_test = X_tokenizer.sequences_to_matrix(X_test, mode='tfidf')\n",
    "\n",
    "    # encode from string labels to numerical labels \n",
    "    label_encoder = LabelEncoder()\n",
    "    y_train = label_encoder.fit_transform(y_train_df.values.astype(str)) # error without astype(str)\n",
    "    y_test = label_encoder.transform(y_test_df.values.astype(str))\n",
    "\n",
    "    y_train = to_categorical(y_train, num_classes)\n",
    "    y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "    # pad character sequences to have the same length\n",
    "    X_train = sequence.pad_sequences(X_train, padding=\"post\", maxlen=maxlen)\n",
    "    X_test = sequence.pad_sequences(X_test, padding=\"post\", maxlen=maxlen)\n",
    "    \n",
    "    max_features = len(X_tokenizer.word_counts)\n",
    "    \n",
    "    return [X_train, y_train, X_test, y_test, max_features, num_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, y_train,\n",
    "          X_test, y_test,\n",
    "          max_features,\n",
    "          maxlen,\n",
    "          num_classes,\n",
    "          nn_type='simple_rnn',\n",
    "          embedding_dims = 50,\n",
    "          epochs=20,\n",
    "          batch_size = 23,\n",
    "         verbose=0):\n",
    "    \n",
    "#     print('Build model...')\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features,\n",
    "                        embedding_dims,\n",
    "                        input_length=maxlen))\n",
    "    if nn_type == 'simple_rnn':\n",
    "        model.add(SimpleRNN(embedding_dims))\n",
    "    elif nn_type == 'lstm':\n",
    "        model.add(LSTM(maxlen))\n",
    "    \n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "#     print(model.summary())\n",
    "#     print('Train model...')\n",
    "\n",
    "    history = model.fit(X_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(X_test, y_test),\n",
    "                verbose=verbose\n",
    "             )\n",
    "    score, acc = model.evaluate(X_test, y_test,\n",
    "                                batch_size=batch_size,\n",
    "                               verbose=verbose)\n",
    "\n",
    "    print('Test model score:', score)\n",
    "    print('Test model accuracy:', acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 30\n",
    "\n",
    "tuning_list = [\n",
    "    {    'name': 'simple_rnn',\n",
    "        'use_bigram': False,\n",
    "        'maxlen': MAX_LEN,\n",
    "        'nn_type': 'simple_rnn'\n",
    "    },\n",
    "    {\n",
    "        'name': 'lstm',\n",
    "        'use_bigram': False,\n",
    "        'maxlen': MAX_LEN,\n",
    "        'nn_type': 'lstm'\n",
    "    },\n",
    "    {\n",
    "        'name': 'simple_rnn_with_bigram',\n",
    "        'use_bigram': True,\n",
    "        'maxlen': MAX_LEN,\n",
    "        'nn_type': 'simple_rnn'\n",
    "    },\n",
    "    {\n",
    "        'name': 'lstm_with_bigram',\n",
    "        'use_bigram': True,\n",
    "        'maxlen': MAX_LEN,\n",
    "        'nn_type': 'lstm'\n",
    "    }\n",
    "]\n",
    "\n",
    "@jit()\n",
    "def main():\n",
    "    for params in tuning_list:\n",
    "        print('##### {} #####'.format(params['name']))\n",
    "        [X_train, y_train, X_test, y_test, max_features, num_classes] = prepare(\n",
    "            maxlen=params['maxlen'], use_bigram=params['use_bigram'])\n",
    "        model(nn_type=params['nn_type'],\n",
    "              X_train=X_train, y_train=y_train, \n",
    "              X_test=X_test, y_test=y_test, \n",
    "              max_features=max_features, \n",
    "              num_classes=num_classes, \n",
    "              maxlen=params['maxlen'], \n",
    "              verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-43-62f95a77c109>:29: NumbaWarning: \n",
      "Compilation is falling back to object mode WITH looplifting enabled because Function \"main\" failed type inference due to: Untyped global name 'tuning_list': Cannot type list element of <class 'dict'>\n",
      "\n",
      "File \"<ipython-input-43-62f95a77c109>\", line 31:\n",
      "def main():\n",
      "    for params in tuning_list:\n",
      "    ^\n",
      "\n",
      "  @jit()\n",
      "<ipython-input-43-62f95a77c109>:29: NumbaWarning: \n",
      "Compilation is falling back to object mode WITHOUT looplifting enabled because Function \"main\" failed type inference due to: cannot determine Numba type of <class 'numba.core.dispatcher.LiftedLoop'>\n",
      "\n",
      "File \"<ipython-input-43-62f95a77c109>\", line 31:\n",
      "def main():\n",
      "    for params in tuning_list:\n",
      "    ^\n",
      "\n",
      "  @jit()\n",
      "/Users/goople/workspace/slu/spring2020/hpc/ml-project/env/lib/python3.7/site-packages/numba/core/object_mode_passes.py:178: NumbaWarning: Function \"main\" was compiled in object mode without forceobj=True, but has lifted loops.\n",
      "\n",
      "File \"<ipython-input-43-62f95a77c109>\", line 31:\n",
      "def main():\n",
      "    for params in tuning_list:\n",
      "    ^\n",
      "\n",
      "  state.func_ir.loc))\n",
      "/Users/goople/workspace/slu/spring2020/hpc/ml-project/env/lib/python3.7/site-packages/numba/core/object_mode_passes.py:188: NumbaDeprecationWarning: \n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
      "\n",
      "For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\n",
      "File \"<ipython-input-43-62f95a77c109>\", line 31:\n",
      "def main():\n",
      "    for params in tuning_list:\n",
      "    ^\n",
      "\n",
      "  state.func_ir.loc))\n",
      "<ipython-input-43-62f95a77c109>:29: NumbaWarning: \n",
      "Compilation is falling back to object mode WITHOUT looplifting enabled because Function \"main\" failed type inference due to: Untyped global name 'tuning_list': Cannot type list element of <class 'dict'>\n",
      "\n",
      "File \"<ipython-input-43-62f95a77c109>\", line 31:\n",
      "def main():\n",
      "    for params in tuning_list:\n",
      "    ^\n",
      "\n",
      "  @jit()\n",
      "/Users/goople/workspace/slu/spring2020/hpc/ml-project/env/lib/python3.7/site-packages/numba/core/object_mode_passes.py:178: NumbaWarning: Function \"main\" was compiled in object mode without forceobj=True.\n",
      "\n",
      "File \"<ipython-input-43-62f95a77c109>\", line 31:\n",
      "def main():\n",
      "    for params in tuning_list:\n",
      "    ^\n",
      "\n",
      "  state.func_ir.loc))\n",
      "/Users/goople/workspace/slu/spring2020/hpc/ml-project/env/lib/python3.7/site-packages/numba/core/object_mode_passes.py:188: NumbaDeprecationWarning: \n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour.\n",
      "\n",
      "For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\n",
      "File \"<ipython-input-43-62f95a77c109>\", line 31:\n",
      "def main():\n",
      "    for params in tuning_list:\n",
      "    ^\n",
      "\n",
      "  state.func_ir.loc))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### simple_rnn #####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/goople/workspace/slu/spring2020/hpc/ml-project/env/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test model score: 0.962059650546626\n",
      "Test model accuracy: 0.48701754212379456\n",
      "##### lstm #####\n",
      "Test model score: 1.0375396541963544\n",
      "Test model accuracy: 0.43578946590423584\n",
      "##### simple_rnn_with_bigram #####\n",
      "Test model score: 1.0644724569822612\n",
      "Test model accuracy: 0.4014035165309906\n",
      "##### lstm_with_bigram #####\n",
      "Test model score: 1.0644768418345536\n",
      "Test model accuracy: 0.4014035165309906\n",
      "CPU times: user 6min 14s, sys: 1min 39s, total: 7min 54s\n",
      "Wall time: 3min 12s\n"
     ]
    }
   ],
   "source": [
    "%time _ = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### simple_rnn #####\n",
      "Test model score: 0.9690599723447834\n",
      "Test model accuracy: 0.48561403155326843\n",
      "##### lstm #####\n",
      "Test model score: 1.038977464876677\n",
      "Test model accuracy: 0.4350877106189728\n",
      "##### simple_rnn_with_bigram #####\n",
      "Test model score: 1.0637121812502544\n",
      "Test model accuracy: 0.4014035165309906\n",
      "##### lstm_with_bigram #####\n",
      "Test model score: 1.064961125767022\n",
      "Test model accuracy: 0.4014035165309906\n",
      "CPU times: user 6min 9s, sys: 1min 34s, total: 7min 44s\n",
      "Wall time: 3min 7s\n"
     ]
    }
   ],
   "source": [
    "%time _ = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
